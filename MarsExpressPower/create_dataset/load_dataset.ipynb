{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "\n",
    "Load the dataset of the Mars Express Power Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pandas import Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run download_dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we donwload the dataset and unzip it inside the folder\n",
    "download_unzip(url, file_name + file_extension, folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The credit for most of the code in this section goes to Alexander Bauer, who very kindly released a baseline script for the competition on github:\n",
    "\n",
    "* https://github.com/alex-bauer/kelvin-power-challenge\n",
    "* [Kelvins - Discussions / Tutorial: SAAF+LTDATA Random Forest baseline script (0.12 PLB)](https://kelvins.esa.int/mars-express-power-challenge/discussion/56/)\n",
    "\n",
    "Specifically, it includes the code from his [rf_baseline.py](https://github.com/alex-bauer/kelvin-power-challenge/blob/a968bdb7f759e35b7f0076d794f6cc517c0dd1b3/src/rf_baseline.py) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to convert the utc timestamp to datetime\n",
    "def convert_time(df):\n",
    "    df['ut_ms'] = pd.to_datetime(df['ut_ms'], unit='ms')\n",
    "    return df\n",
    "\n",
    "# Function to resample the dataframe to hourly mean\n",
    "def resample_1H(df):\n",
    "    df = df.set_index('ut_ms')\n",
    "    df = df.resample('1H').mean()\n",
    "    return df\n",
    "\n",
    "# Function to read a csv file and resample to hourly consumption\n",
    "def parse_ts(filename, dropna=True):\n",
    "    df = pd.read_csv(PATH_TO_DATA + '/' + filename)\n",
    "    df = convert_time(df)\n",
    "    df = resample_1H(df)\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# Function to read the ltdata files\n",
    "def parse_ltdata(filename):\n",
    "    df = pd.read_csv(PATH_TO_DATA + '/' + filename)\n",
    "    df = convert_time(df)\n",
    "    df = df.set_index('ut_ms')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the power files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cguz/.local/lib/python3.7/site-packages/ipykernel_launcher.py:2: DtypeWarning: Columns (18) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "## Load the power files: they are the columns that need to be predicted (predicted value)\n",
    "pow_train1 = parse_ts('/train_set/power--2008-08-22_2010-07-10.csv')\n",
    "pow_train2 = parse_ts('/train_set/power--2010-07-10_2012-05-27.csv')\n",
    "pow_train3 = parse_ts('/train_set/power--2012-05-27_2014-04-14.csv')\n",
    "\n",
    "# Load the test sample submission file as template for prediction\n",
    "pow_test = parse_ts('power-prediction-sample-2014-04-14_2016-03-01.csv', False)\n",
    "\n",
    "# Concatenate the files\n",
    "power_all = pd.concat([pow_train1, pow_train2, pow_train3, pow_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPWD2372 NPWD2401 NPWD2402 NPWD2451 NPWD2471 NPWD2472 NPWD2481 NPWD2482 NPWD2491 NPWD2501 NPWD2531 NPWD2532 NPWD2551 NPWD2552 NPWD2561 NPWD2562 NPWD2691 NPWD2721 NPWD2722 NPWD2742 NPWD2771 NPWD2791 NPWD2792 NPWD2801 NPWD2802 NPWD2821 NPWD2851 NPWD2852 NPWD2871 NPWD2872 NPWD2881 NPWD2882 NPWD2692\n"
     ]
    }
   ],
   "source": [
    "# Extract the columns that need to be predicted\n",
    "power_cols = list(power_all.columns)\n",
    "print('power cols')\n",
    "print(' '.join(power_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load SAAF files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the saaf files\n",
    "saaf_train1 = parse_ts('/train_set/context--2008-08-22_2010-07-10--saaf.csv')\n",
    "saaf_train2 = parse_ts('/train_set/context--2010-07-10_2012-05-27--saaf.csv')\n",
    "saaf_train3 = parse_ts('/train_set/context--2012-05-27_2014-04-14--saaf.csv')\n",
    "saaf_test = parse_ts('/test_set/context--2014-04-14_2016-03-01--saaf.csv')\n",
    "saaf_all = pd.concat([saaf_train1, saaf_train2, saaf_train3, saaf_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sa sx sy sz\n"
     ]
    }
   ],
   "source": [
    "# Extract the columns name\n",
    "saaf_cols = list(saaf_all.columns)\n",
    "print('saaf cols')\n",
    "print(' '.join(saaf_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load LTDATA files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the ltdata files\n",
    "ltdata_train1 = parse_ltdata('/train_set/context--2008-08-22_2010-07-10--ltdata.csv')\n",
    "ltdata_train2 = parse_ltdata('/train_set/context--2010-07-10_2012-05-27--ltdata.csv')\n",
    "ltdata_train3 = parse_ltdata('/train_set/context--2012-05-27_2014-04-14--ltdata.csv')\n",
    "ltdata_test = parse_ltdata('/test_set/context--2014-04-14_2016-03-01--ltdata.csv')\n",
    "ltdata_all = pd.concat([ltdata_train1, ltdata_train2, ltdata_train3, ltdata_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunmars_km earthmars_km sunmarsearthangle_deg solarconstantmars eclipseduration_min occultationduration_min\n"
     ]
    }
   ],
   "source": [
    "# Extract the columns name\n",
    "ltdata_cols = list(ltdata_all.columns)\n",
    "print('ltdata cols')\n",
    "print(' '.join(ltdata_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load DMOP files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_dmop(filename):\n",
    "    df = pd.read_csv(PATH_TO_DATA + '/' + filename)\n",
    "    df = convert_time(df)\n",
    "\n",
    "    subsystems = sorted({s[1:4] for s in df['subsystem'] if s[0] == 'A'})\n",
    "    \n",
    "    df_expansion = [\n",
    "        [when] + [(1 if cmd[1:4]==syst else 0) for syst in subsystems]\n",
    "        for (when, cmd) in df.values\n",
    "        if cmd[0]=='A']\n",
    "    \n",
    "    df = pd.DataFrame(df_expansion, columns=['ut_ms'] + subsystems)\n",
    "    df = df.set_index('ut_ms')\n",
    "    \n",
    "    # get one row per hour, containing the boolean values indicating whether each\n",
    "    # subsystem was activated in that hour;\n",
    "    # hours not represented in the file have all columns with 0.\n",
    "    # df = df.resample('1H').max().fillna(0)\n",
    "    # cells represent number of times intructions were issued to that subsystem in that hour\n",
    "    df = df.resample('1H').sum().fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dmop files\n",
    "dmop_train1 = parse_dmop('/train_set/context--2008-08-22_2010-07-10--dmop.csv')\n",
    "dmop_train2 = parse_dmop('/train_set/context--2010-07-10_2012-05-27--dmop.csv')\n",
    "dmop_train3 = parse_dmop('/train_set/context--2012-05-27_2014-04-14--dmop.csv')\n",
    "dmop_test = parse_dmop('/test_set/context--2014-04-14_2016-03-01--dmop.csv')\n",
    "dmop_all = pd.concat([dmop_train1, dmop_train2, dmop_train3, dmop_test]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAA HHH MMM OOO PSF PWF SEQ SSS SXX TMB TTT VVV XXX ACF DMC DMF\n"
     ]
    }
   ],
   "source": [
    "# Extract the columns name\n",
    "dmop_cols = list(dmop_all.columns)\n",
    "print('dmop cols')\n",
    "print(' '.join(dmop_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load FTL files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create df to join everything together\n",
    "df = power_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_ftl(filename):\n",
    "    df = pd.read_csv(PATH_TO_DATA + '/' + filename)\n",
    "    df['utb_ms'] = pd.to_datetime(df['utb_ms'], unit='ms')\n",
    "    df['ute_ms'] = pd.to_datetime(df['ute_ms'], unit='ms')\n",
    "    return df\n",
    "\n",
    "def parse_ftl_all(filenames, hour_indices):\n",
    "    ftl_all = pd.concat([parse_ftl(f) for f in filenames])\n",
    "    \n",
    "    types = sorted(set(ftl_all['type']))\n",
    "    ftl_df = pd.DataFrame(index=hour_indices, columns=['flagcomms'] + types).fillna(0)\n",
    "    \n",
    "    # hour indices of discarded events, because of non-ocurrence in `hour_indices`\n",
    "    ix_err = []\n",
    "\n",
    "    for (t_start, t_end, p_type, comms) in ftl_all.values:\n",
    "        floor_beg = Timestamp(t_start).floor('1h')\n",
    "        floor_end = Timestamp(t_end).floor('1h')\n",
    "        \n",
    "        try:\n",
    "            ftl_df.loc[floor_beg]['flagcomms'] = ftl_df.loc[floor_end]['flagcomms'] = int(comms)\n",
    "            ftl_df.loc[floor_beg][p_type]      = ftl_df.loc[floor_end][p_type]      = 1\n",
    "        except KeyError:\n",
    "            ix_err.append((floor_beg, floor_end))\n",
    "    \n",
    "    print('Warning: discarded %d FTL events' % len(ix_err))\n",
    "    \n",
    "    return ftl_all, ftl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: discarded 16338 FTL events\n",
      "CPU times: user 53.3 s, sys: 30.9 ms, total: 53.3 s\n",
      "Wall time: 53.3 s\n"
     ]
    }
   ],
   "source": [
    "ftl_fnames = [\n",
    "    '/train_set/context--2008-08-22_2010-07-10--ftl.csv',\n",
    "    '/train_set/context--2010-07-10_2012-05-27--ftl.csv',\n",
    "    '/train_set/context--2012-05-27_2014-04-14--ftl.csv',\n",
    "    '/test_set/context--2014-04-14_2016-03-01--ftl.csv',\n",
    "    ]\n",
    "%time ftl_all, ftl_df = parse_ftl_all(filenames=ftl_fnames, hour_indices=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utb_ms ute_ms type flagcomms\n"
     ]
    }
   ],
   "source": [
    "# Extract the columns name\n",
    "ftl_cols = list(ftl_all.columns)\n",
    "print('ftl cols')\n",
    "print(' '.join(ftl_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join cols names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = saaf_cols + ltdata_cols + dmop_cols + ftl_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sa', 'sx', 'sy', 'sz', 'sunmars_km', 'earthmars_km', 'sunmarsearthangle_deg', 'solarconstantmars', 'eclipseduration_min', 'occultationduration_min', 'AAA', 'HHH', 'MMM', 'OOO', 'PSF', 'PWF', 'SEQ', 'SSS', 'SXX', 'TMB', 'TTT', 'VVV', 'XXX', 'ACF', 'DMC', 'DMF', 'utb_ms', 'ute_ms', 'type', 'flagcomms']\n"
     ]
    }
   ],
   "source": [
    "# print(all_cols)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00e021f687322540f4544787d6c52e8d6f687ec79fd3ada94c51037603920421"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
